@inproceedings{chaganty2023beyond,
  title={Beyond single items: Exploring user preferences in item sets with the conversational playlist curation dataset},
  author={Chaganty, Arun Tejasvi and Leszczynski, Megan and Zhang, Shu and Ganti, Ravi and Balog, Krisztian and Radlinski, Filip},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2754--2764},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  year={2022}
}

@inproceedings{manco2022learning,
  title={Learning music audio representations via weak language supervision},
  author={Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, Gy{\"o}rgy},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={456--460},
  year={2022},
  organization={IEEE}
}

@inproceedings{choi2019zero,
  title={Zero-shot learning for audio-based music classification and tagging},
  author={Choi, Jeong and Lee, Jongpil and Park, Jiyoung and Nam, Juhan},
  booktitle={ISMIR},
  year      = {2019}
}

@article{turnbull2008semantic,
  title={Semantic annotation and retrieval of music and sound effects},
  author={Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={16},
  number={2},
  pages={467--476},
  year={2008},
  publisher={IEEE}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{manco2023song,
  title={The Song Describer Dataset: A corpus of audio captions for music-and-language evaluation},
  author={Manco, Ilaria and Weck, Benno and Doh, Seungheon and Won, Minz and Zhang, Yixiao and Bodganov, Dmitry and Wu, Yusong and Chen, Ke and Tovstogan, Philip and Benetos, Emmanouil and others},
  journal={arXiv preprint arXiv:2311.10057},
  year={2023}
}

@article{agostinelli2023musiclm,
  title={Musiclm: Generating music from text},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}

@article{wu2023music,
  title={Music ControlNet: Multiple time-varying controls for music generation},
  author={Wu, Shih-Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J},
  journal={arXiv preprint arXiv:2311.07069},
  year={2023}
}

@article{novack2024ditto,
  title={DITTO: Diffusion Inference-Time T-Optimization for Music Generation},
  author={Novack, Zachary and McAuley, Julian and Berg-Kirkpatrick, Taylor and Bryan, Nicholas J},
  journal={arXiv preprint arXiv:2401.12179},
  year={2024}
}

@article{copet2024simple,
  title={Simple and controllable music generation},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chen2023musicldm,
  title={MusicLDM: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies},
  author={Chen, Ke and Wu, Yusong and Liu, Haohe and Nezhurina, Marianna and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
  journal={arXiv preprint arXiv:2308.01546},
  year={2023}
}

@inproceedings{doh2023toward,
  title={Toward universal text-to-music retrieval},
  author={Doh, SeungHeon and Won, Minz and Choi, Keunwoo and Nam, Juhan},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{manco2021muscaps,
  title={Muscaps: Generating captions for music audio},
  author={Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, Gy{\"o}rgy},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@article{gardner2023llark,
  title={Llark: A multimodal foundation model for music},
  author={Gardner, Josh and Durand, Simon and Stoller, Daniel and Bittner, Rachel M},
  journal={arXiv preprint arXiv:2310.07160},
  year={2023}
}

@article{doh2023lp,
  title={Lp-musiccaps: Llm-based pseudo music captioning},
  author={Doh, SeungHeon and Choi, Keunwoo and Lee, Jongpil and Nam, Juhan},
  journal={arXiv preprint arXiv:2307.16372},
  year={2023}
}

@article{manco2022contrastive,
  title={Contrastive audio-language learning for music},
  author={Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, Gy{\"o}rgy},
  journal={arXiv preprint arXiv:2208.12208},
  year={2022}
}

@article{huang2022mulan,
  title={Mulan: A joint embedding of music audio and natural language},
  author={Huang, Qingqing and Jansen, Aren and Lee, Joonseok and Ganti, Ravi and Li, Judith Yue and Ellis, Daniel PW},
  journal={arXiv preprint arXiv:2208.12415},
  year={2022}
}

@article{dhariwal2020jukebox,
  title={Jukebox: A generative model for music},
  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2005.00341},
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={8821--8831},
  year={2021},
  organization={Pmlr}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@inproceedings{engel2017neural,
  title={Neural audio synthesis of musical notes with wavenet autoencoders},
  author={Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
  booktitle={International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}

@article{mor2018universal,
  title={A universal music translation network},
  author={Mor, Noam and Wolf, Lior and Polyak, Adam and Taigman, Yaniv},
  journal={arXiv preprint arXiv:1805.07848},
  year={2018}
}

@article{donahue2018adversarial,
  title={Adversarial audio synthesis},
  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
  journal={arXiv preprint arXiv:1802.04208},
  year={2018}
}

@article{mehri2016samplernn,
  title={SampleRNN: An unconditional end-to-end neural audio generation model},
  author={Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1612.07837},
  year={2016}
}

@article{van2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Van Den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray and others},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@article{fu2010survey,
  title={A survey of audio-based music classification and annotation},
  author={Fu, Zhouyu and Lu, Guojun and Ting, Kai Ming and Zhang, Dengsheng},
  journal={IEEE transactions on multimedia},
  year={2010},
  publisher={IEEE}
}

@inproceedings{sordo2007annotating,
  title={Annotating Music Collections: How Content-Based Similarity Helps to Propagate Labels.},
  author={Sordo, Mohamed and Laurier, Cyril and Celma, Oscar},
  booktitle={ISMIR},
  pages={531--534},
  year={2007}
}

@article{eck2007automatic,
  title={Automatic generation of social tags for music recommendation},
  author={Eck, Douglas and Lamere, Paul and Bertin-Mahieux, Thierry and Green, Stephen},
  journal={Advances in neural information processing systems},
  year={2007}
}

@article{lamere2008social,
  title={Social tagging and music information retrieval},
  author={Lamere, Paul},
  journal={Journal of new music research},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{nam2018deep,
  title={Deep learning for audio-based music classification and tagging: Teaching computers to distinguish rock from bach},
  author={Nam, Juhan and Choi, Keunwoo and Lee, Jongpil and Chou, Szu-Yu and Yang, Yi-Hsuan},
  journal={IEEE signal processing magazine},
  year={2018},
  publisher={IEEE}
}

@article{tzanetakis2002musical,
  author={Tzanetakis, G. and Cook, P.},
  journal={IEEE Transactions on Speech and Audio Processing}, 
  title={Musical genre classification of audio signals}, 
  year={2002},
  volume={10},
  number={5},
  pages={293-302},
  keywords={Humans;Music information retrieval;Instruments;Computer science;Multiple signal classification;Signal analysis;Pattern recognition;Feature extraction;Wavelet analysis;Cultural differences},
  doi={10.1109/TSA.2002.800560}}

@inproceedings{kim2010music,
  title={Music emotion recognition: A state of the art review},
  author={Kim, Youngmoo E and Schmidt, Erik M and Migneco, Raymond and Morton, Brandon G and Richardson, Patrick and Scott, Jeffrey and Speck, Jacquelin A and Turnbull, Douglas},
  booktitle={Proc. ismir},
  volume={86},
  pages={937--952},
  year={2010}
}


@article{herrera2003automatic,
    author = {Perfecto Herrera-Boyer, Geoffroy Peeters and Shlomo Dubnov},
    title = {Automatic Classification of Musical Instrument Sounds},
    journal = {Journal of New Music Research},
    volume = {32},
    number = {1},
    pages = {3--21},
    year = {2003},
    publisher = {Routledge},
    doi = {10.1076/jnmr.32.1.3.16798},
}

@inproceedings{gabbolini-etal-2022-data,
    title = "Data-Efficient Playlist Captioning With Musical and Linguistic Knowledge",
    author = "Gabbolini, Giovanni  and
      Hennequin, Romain  and
      Epure, Elena",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.784",
    doi = "10.18653/v1/2022.emnlp-main.784",
    pages = "11401--11415",
}

@inproceedings{mckee2023language,
  author={McKee, Daniel and Salamon, Justin and Sivic, Josef and Russell, Bryan},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Language-Guided Music Recommendation for Video via Prompt Analogies}, 
  year={2023},
  volume={},
  number={},
  pages={14784-14793},
  keywords={Training;Visualization;Fuses;Natural languages;Training data;Music;Data models;Multi-modal learning},
  doi={10.1109/CVPR52729.2023.01420}}

@inproceedings{choi2016towards,
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
booktitle={Extended abstracts for the Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference}, 
year = {2016},
month = {08},
pages = {},
title = {Towards Music Captioning: Generating Music Playlist Descriptions},
doi = {10.48550/arXiv.1608.04868}
}

@inproceedings{choi2017convolutional,
  author={Choi, Keunwoo and Fazekas, Gy√∂rgy and Sandler, Mark and Cho, Kyunghyun},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Convolutional recurrent neural networks for music classification}, 
  year={2017},
  volume={},
  number={},
  pages={2392-2396},
  keywords={Convolution;Feature extraction;Kernel;Tagging;Training;Two dimensional displays;Recurrent neural networks;convolutional neural networks;recurrent neural networks;music classification},
  doi={10.1109/ICASSP.2017.7952585}
  }

@article{lee2017multilevel,
  author={Lee, Jongpil and Nam, Juhan},
  journal={IEEE Signal Processing Letters}, 
  title={Multi-Level and Multi-Scale Feature Aggregation Using Pretrained Convolutional Neural Networks for Music Auto-Tagging}, 
  year={2017},
  volume={24},
  number={8},
  pages={1208-1212},
  keywords={Feature extraction;Convolution;Neural networks;Predictive models;Spectrogram;Multiple signal classification;Aggregates;Convolutional neural networks;feature aggregation;music auto-tagging;transfer learning},
  doi={10.1109/LSP.2017.2713830}
  }

@inproceedings{won2021transformer,
  title={Semi-supervised music tagging transformer},
  author={Won, Minz and Choi, Keunwoo and Serra, Xavier},
  booktitle={Proc. of International Society for Music Information Retrieval},
  year={2021}
}

@inproceedings{tang_salmonn_2024,
	title = {{SALMONN}: {Towards} {Generic} {Hearing} {Abilities} for {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=14rn7HpKVk},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and MA, Zejun and Zhang, Chao},
	year = {2024},
}

@inproceedings{liu_music_2024,
  author={Liu, Shansong and Hussain, Atin Sakkeer and Sun, Chenshuo and Shan, Ying},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning}, 
  year={2024},
  volume={},
  number={},
  pages={286-290},
  keywords={Training;Measurement;Natural languages;Tagging;Signal processing;Feature extraction;Question answering (information retrieval);MU-LLaMA;MusicQA dataset;music question answering;text-to-music generation},
  doi={10.1109/ICASSP48485.2024.10447027}
}

@article{li_music_2024,
  title={The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models},
  author={Li, Jiajia and Yang, Lu and Tang, Mingni and Chen, Cong and Li, Zuchao and Wang, Ping and Zhao, Hai},
  journal={arXiv preprint arXiv:2406.15885},
  year={2024}
}

@inproceedings{deng_musilingo_2024,
	address = {Mexico City, Mexico},
	title = {{MusiLingo}: {Bridging} {Music} and {Text} with {Pre}-trained {Language} {Models} for {Music} {Captioning} and {Query} {Response}},
	shorttitle = {{MusiLingo}},
	url = {https://aclanthology.org/2024.findings-naacl.231},
	abstract = {Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains not well-explored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT (CITATION) with a frozen LLM, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q\&A datasets, we created the MusicInstruct (MI) dataset from captions in the MusicCaps datasets, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q\&A pairs. Our introduced dataset enables notable advancements beyond previous ones.},
	urldate = {2024-07-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Deng, Zihao and Ma, Yinghao and Liu, Yudong and Guo, Rongchen and Zhang, Ge and Chen, Wenhu and Huang, Wenhao and Benetos, Emmanouil},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {3643--3655},
}

@inproceedings{weck_muchomusic_2024,
	title = {{MuChoMusic}: {Evaluating} {Music} {Understanding} in {Multimodal} {Audio}-{Language} {Models}},
	shorttitle = {{MuChoMusic}},
	url = {http://arxiv.org/abs/2408.01337},
	doi = {10.48550/arXiv.2408.01337},
	urldate = {2024-08-21},
	booktitle = {25th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Weck, Benno and Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, George and Bogdanov, Dmitry},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01337 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{hussain2023m,
  title={{M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models}},
  author={Hussain, Atin Sakkeer and Liu, Shansong and Sun, Chenshuo and Shan, Ying},
  journal={arXiv preprint arXiv:2311.11255},
  year={2023}
}

@inproceedings{tang_salmonn_2023,
	title = {{SALMONN}: {Towards} {Generic} {Hearing} {Abilities} for {Large} {Language} {Models}},
	shorttitle = {{SALMONN}},
	url = {https://openreview.net/forum?id=14rn7HpKVk},
	urldate = {2024-02-22},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
	month = oct,
	year = {2023},
}

@inproceedings{deshmukh_pengi_2023,
	title = {Pengi: {An} {Audio} {Language} {Model} for {Audio} {Tasks}},
	shorttitle = {Pengi},
	url = {http://arxiv.org/abs/2305.11834},
	doi = {10.48550/arXiv.2305.11834},
	urldate = {2024-02-16},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
	year = {2023},
	note = {arXiv:2305.11834 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{chu_qwen-audio_2023,
	title = {Qwen-{Audio}: {Advancing} {Universal} {Audio} {Understanding} via {Unified} {Large}-{Scale} {Audio}-{Language} {Models}},
	shorttitle = {Qwen-{Audio}},
	url = {http://arxiv.org/abs/2311.07919},
	doi = {10.48550/arXiv.2311.07919},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren},
	month = dec,
	year = {2023},
	note = {arXiv:2311.07919 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{wu2024futga,
  title={Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation},
  author={Wu, Junda and Novack, Zachary and Namburi, Amit and Dai, Jiaheng and Dong, Hao-Wen and Xie, Zhouhang and Chen, Carol and McAuley, Julian},
  journal={arXiv preprint arXiv:2407.20445},
  year={2024}
}

@inproceedings{gao_music_2023,
	title = {Music {Question} {Answering} {Based} on {Aesthetic} {Experience}},
	url = {https://ieeexplore.ieee.org/abstract/document/10191775},
	doi = {10.1109/IJCNN54540.2023.10191775},
	abstract = {Music understanding has always been considered to be the work of experts. Ordinary people have insufficient aesthetic experience when facing music. We put forward the topic of music question and answer based on aesthetic experience to help people understand music more comprehensively. We summarized the relevant external characteristics of music elements that people are most concerned about in the field of music analysis, and generated a structured music aesthetic experience database. Based on this, we constructed the AMQA-dataset. We propose a new method of music graph representation, which can improve the reasonability and interpretability of music question answering system. We have verified our idea on the AMQA dataset, and achieved experimental results comparable to those of humans.},
	urldate = {2024-03-01},
	booktitle = {2023 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Gao, Wenhao and Li, Xiaobing and Tie, Yun and Qi, Lin},
	month = jun,
	year = {2023},
	note = {ISSN: 2161-4407},
	keywords = {Databases, Neural networks, Question answering (information retrieval)},
	pages = {01--06},
    }

@article{musicgenerationtemplate,
  title={Music Generation Reference Template},
  author={Chen, Ke and Novack, Zachary},
  journal={UCSD},
  year={2024},
  publisher={University of California}
}
