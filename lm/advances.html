
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Advances &#8212; Connecting Music Audio and Natural Language</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lm/advances';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Challenges" href="challenges.html" />
    <link rel="prev" title="The Framework" href="framework.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Connecting Music Audio and Natural Language - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Connecting Music Audio and Natural Language - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Connecting Music Audio and Natural Language
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 1. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction/background.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/overview.html">Overview of Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/advantange.html">Why Natural Langauge?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 2. Overview of Language Model</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="framework.html">The Framework</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Advances</a></li>
<li class="toctree-l1"><a class="reference internal" href="challenges.html">Challenges</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 3. Music Description</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../description/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../description/tasks.html">Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../description/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../description/datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../description/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../description/code.html">Code Practice</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 4. Text-to-Music Retrieval</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../retrieval/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retrieval/models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retrieval/evaluate.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retrieval/code.html">Code Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retrieval/challenge.html">Challenges</a></li>
<li class="toctree-l1"><a class="reference internal" href="../retrieval/conversational_retrieval.html">Conversational Retrieval</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 5. Text-to-Music Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../generation/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generation/evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generation/lmmodel.html">MusicGEN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generation/diffusionmodel.html">Diffusion Model-based Text-to-Music Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generation/code.html">Code Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 6. Conclusion</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../conclusion/intro.html">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conclusion/beyondaudio.html">Beyond Audio Modality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conclusion/beyondtext.html">Beyond Text-Based Interactions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/mulab-mir/music-language-tutorial" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/mulab-mir/music-language-tutorial/issues/new?title=Issue%20on%20page%20%2Flm/advances.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lm/advances.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advances</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-from-language-models">Transfer Learning from Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-task-transfer-and-in-context-learning">Zero-shot Task Transfer and In-Context Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-laws-of-language-models">Scaling Laws of Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distillation-of-language-models">Distillation of Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aligning-language-models-with-human-feedback">Aligning Language Models with Human Feedback</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-use-and-function-calling">Tool Use and Function Calling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-encoders-for-language-model-inputs">Multimodal Encoders for Language Model Inputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-decoders-for-language-model-outputs">Multimodal Decoders for Language Model Outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-reasoning-of-language-models">Chain-of-Thought Reasoning of Language Models</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advances">
<h1>Advances<a class="headerlink" href="#advances" title="Link to this heading">#</a></h1>
<p>In the previous section, we introduced language modeling as a powerful machine learning framework for learning the mapping between any inputs and outputs:</p>
<p><img alt="the-framework" src="../_images/the-framework.png" /></p>
<p>In this section, we identify a few limitations of using this framework and their solutions.</p>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h2>
<p>While this framework is really powerful, in practice, it of course comes with a variety of limitations. The table below lists some of those limitations and how the field has been addressing each of them.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Limitation</p></th>
<th class="head text-center"><p>so it’s difficult to …</p></th>
<th class="head text-center"><p>but we can …</p></th>
<th class="head text-center"><p>Solution</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>It’s a generative model</p></td>
<td class="text-center"><p>use for discriminative tasks</p></td>
<td class="text-center"><p>use probes or fine-tune on each task</p></td>
<td class="text-center"><p>Transfer learning</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>It generates nonsense</p></td>
<td class="text-center"><p>use the model</p></td>
<td class="text-center"><p>use more data / bigger Transformers</p></td>
<td class="text-center"><p>Scaling</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Now it’s is too huge and slow</p></td>
<td class="text-center"><p>generate quickly and cheaply</p></td>
<td class="text-center"><p>distill a small model from a large one</p></td>
<td class="text-center"><p>Distillation</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>It doesn’t behave as desired</p></td>
<td class="text-center"><p>use as a helpful assistant</p></td>
<td class="text-center"><p>fine-tune for human preference</p></td>
<td class="text-center"><p>Post-Training / RLHF</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Its context is limited</p></td>
<td class="text-center"><p>work with larger search spaces</p></td>
<td class="text-center"><p>query external sources for more context</p></td>
<td class="text-center"><p>RAG / Tool Use / Function Calling</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Multimodal data is huge</p></td>
<td class="text-center"><p>hard to use as condition</p></td>
<td class="text-center"><p>use an encoder, e.g. CLIP and Whisper</p></td>
<td class="text-center"><p>Multimodal Encoders</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Outputs have to be discrete</p></td>
<td class="text-center"><p>sample continuous data</p></td>
<td class="text-center"><p>use a VQ-VAE and/or diffusion</p></td>
<td class="text-center"><p>Multimodal Decoders</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Constant compute per token</p></td>
<td class="text-center"><p>make it think deeper when needed</p></td>
<td class="text-center"><p>make the model think step-by-step</p></td>
<td class="text-center"><p>Chain-of-Thought Reasoning</p></td>
</tr>
</tbody>
</table>
</div>
<p>As you can see, each of these limitations opens a whole literature of research areas surrounding language models. In the following, I will briefly introduce each of these topics and related research progresses.</p>
<section id="transfer-learning-from-language-models">
<h3>Transfer Learning from Language Models<a class="headerlink" href="#transfer-learning-from-language-models" title="Link to this heading">#</a></h3>
<p>Once we have a trained language model that gives us a probability distribution of tokens, we can sample some texts and play with it, but how about other kinds of tasks, such as discriminative tasks like classification? This brings us to <strong>Transfer Learning</strong>.</p>
<p>The tasks that we train language models for, such as next-token prediction or filling in the blanks, are called <strong>pretext tasks</strong>, in a sense that they aren’t necessarily the final goal but are useful for learning general representations of language.
So the language models trained this way not only can just generate completions for any given prefix, but also have learned a lot about language structure, patterns, and semantics, that we can make use of.</p>
<p>To do that, we can replace the last layer of the model with a task-specific head, reusing the base part of the model. This head is fine-tuned for the task at hand, while the base model itself can be either updated or frozen, depending on the task and the amount of data we have.</p>
<p>This transfer learning process allows us to leverage the model’s learned representations and apply them flexibly across a wide range of applications, often with minimal task-specific data.</p>
</section>
<section id="zero-shot-task-transfer-and-in-context-learning">
<h3>Zero-shot Task Transfer and In-Context Learning<a class="headerlink" href="#zero-shot-task-transfer-and-in-context-learning" title="Link to this heading">#</a></h3>
<p>When we have a powerful enough generative model, we don’t necessarily have to fine-tune the model for each specific task. We can simply prompt the generative language model with a task description—like <code class="docutils literal notranslate"><span class="pre">translate</span> <span class="pre">English</span> <span class="pre">to</span> <span class="pre">German:</span></code> or <code class="docutils literal notranslate"><span class="pre">summarize</span> <span class="pre">this</span> <span class="pre">paragraph</span></code>, and it will continue with a response based on its pre-existing knowledge.</p>
<p>This approach is called zero-shot task transfer, and it allows us to skip the fine-tuning stage entirely and use the model for various text-to-text tasks without any additional training.
Classification or regression tasks can also be formatted as a text-to-text task, for example by including all multiple-choice options in the prompt with <code class="docutils literal notranslate"><span class="pre">A.</span></code> <code class="docutils literal notranslate"><span class="pre">B.</span></code> <code class="docutils literal notranslate"><span class="pre">C.</span></code> <code class="docutils literal notranslate"><span class="pre">D.</span></code> prefixes and making the model continue with the letter corresponding to the correct answer.</p>
<p>We can also provide a few examples of the task at hand directly in the prompt, to contextualize the model into the mode of solving that specific task. For example, we might provide a few English-to-French translation pairs to help the model recognize the task pattern and provide better translations. This is called <strong>few-shot learning</strong> because the model can adapt quickly from just a few examples of task-specific training data, and the capability of learning the task directly from the prompt without having to fine-tune the model at all is called <strong>in-context learning</strong>.</p>
<p>These realizations happened as we scale up language models from above one billion of parameters in GPT-2, to above ten billion parameters in T5, and above hundred billion parameters in GPT-3, popularizing the term large language models or LLMs. Why do these capabilities “emerge” as we scale up the model?</p>
</section>
<section id="scaling-laws-of-language-models">
<h3>Scaling Laws of Language Models<a class="headerlink" href="#scaling-laws-of-language-models" title="Link to this heading">#</a></h3>
<p>We don’t have a perfect answer for why some emergent capabilities show up at certain scale, but these development brings us to an important concept: the Scaling Laws of Language Models.</p>
<p>Neural networks can be scaled up by adding more layers, channels, or parameters, which often translates into better performance. Researchers have empirically found that, increasing the amount of compute used in training has a predictable effect on reducing the loss. Essentially, by scaling up compute resources, we can achieve lower error rates and improved generalization.</p>
<p><img alt="scaling-laws" src="../_images/scaling-laws.png" /></p>
<p>The graph on the left illustrates that as we increase the compute budget, the test loss continues to decrease, and their pareto frontier is a straight line in a log-log plot, which indicates a power-law relation.</p>
<p>The second plot from the Chinchilla paper shows that for a fixed amount of compute budget, there is an optimal model size and the dataset size that achieve the lowest loss. In the LLM jargon, this size is called Chinchilla-optimal.</p>
<p>The graphs above only concern the training compute, but in practice, significantly more compute resources can be needed for inference, since the model can be used by millions of users after being trained just once. For this reason, language models are often trained for a lot longer than the Chinchilla-optimal point, because having a smaller model that achieves the same performance can save a lot of inference compute, even if it costs more for training.</p>
</section>
<section id="distillation-of-language-models">
<h3>Distillation of Language Models<a class="headerlink" href="#distillation-of-language-models" title="Link to this heading">#</a></h3>
<p>As we alluded above, it is often the situation where scaling up language models helps, but we want smaller models in practice for faster inference and lower cost.
<strong>Knowledge distillation</strong> is a promising method for obtaining a small, “student” model that can perform as well as a big, “teacher”  model.
This is possible because the teacher model can provide richer supervision than just the training data, such as the logits for all probable tokens during prediction, not just the discrete one-hot label that the training data can provide.</p>
<p><img alt="distillation" src="../_images/distillation.png" /></p>
<p>More recently, the term distillation also refers to the process of training on the outputs sampled from a more “intelligent” or bigger model, because the outputs from those models are sufficiently high-quality and large-volume compared to what’s available otherwise. A good example of this is distilling from the outputs of the GPT-4o model for a certain task, where the customers can train a GPT-4o mini model on those outputs and save costs when using the model for that specific task.</p>
</section>
<section id="aligning-language-models-with-human-feedback">
<h3>Aligning Language Models with Human Feedback<a class="headerlink" href="#aligning-language-models-with-human-feedback" title="Link to this heading">#</a></h3>
<p>Because autoregressive language models are just trained to generate continuations, it doesn’t always follow our intention. For example, if we prompt the model with a question, it may answer the question, or continue with a bunch of related questions.</p>
<p>We discussed few-shot learning as a remedy, which provides a few examples in the prompt that follow the desired behavior,
but a more effective way to make the model behave in the way we want is introduced in the InstructGPT paper, called <strong>RLHF</strong>, or <strong>reinforcement learning from human feedback</strong>.
This is a three-step process of fine-tuning the pretrained language model to align with our desired behavior.</p>
<ul class="simple">
<li><p><strong>Step 1: Supervised Fine-Tuning (SFT)</strong> – First, we collect demonstration data. For example, we might ask human labelers to generate responses for certain prompts, showing the model what the desired output behavior looks like. This supervised data is then used to fine-tune the model, which creates a baseline that generally reflects human preferences.</p></li>
<li><p><strong>Step 2: Reward Modeling (RM)</strong> – we then collect comparison data by showing human labelers several model outputs for the same prompt and asking them to rank these responses from best to worst. This data is used to train a separate reward model, that can evaluate and provide reward scores on model outputs showing how closely they align with human preference.</p></li>
<li><p><strong>Step 3: Proximal Policy Optimization (PPO)</strong> – we use reinforcement learning, specifically Proximal Policy Optimization, to adjust the model’s behavior to maximize the reward of its outputs, based on the reward model that we trained in the previous step.</p></li>
</ul>
<p>The central idea of RLHF is to view a language model not just as a text continuation model but a policy in the reinforcement learning context that goes through a multi-step markov decision process of generating a token at each step, and in the and receives a reward signal based on how good the entire response is.
The intuition is that this process allows the model to be fine-tuned more holistically to match human preference, more than just doing a supervised fine-tuning which optimizes for getting the next-token prediction correct.</p>
<p>A more recent method called <strong>direct preference optimization or DPO</strong> is an alternative method to RLHF, which does not involve reinforcement learning but still optimizes for a mathematically equivalent objective as RLHF. DPO was used for fine-tuning LLaMA 2 and LLaMA 3.</p>
<p>This overall process is called <strong>post-training</strong>, because it happens after pre-training the base model and before shipping the language model as a product which requires the model to be aligned with human preferences.</p>
</section>
<section id="retrieval-augmented-generation-rag">
<h3>Retrieval-Augmented Generation (RAG)<a class="headerlink" href="#retrieval-augmented-generation-rag" title="Link to this heading">#</a></h3>
<p>Another challenge in using language models is that they can only generate responses based on the data they were trained on, which means they might lack up-to-date or specialized information.
To address this, we can include new relevant information in the prompt, as we discussed regarding few-shot learning and in-context learning, but the context length of a language model is limited.</p>
<p>This is where <strong>Retrieval-Augmented Generation (RAG)</strong> comes in as a powerful approach.
RAG enables the model to search for relevant information from external sources—such as a knowledge base, recent news articles, or an internal database of a company—and incorporate this information into its response, even when such information was not in the training dataset.</p>
<p><img alt="RAG" src="../_images/rag.png" /></p>
<p>The process works as follows:</p>
<ol class="arabic simple">
<li><p>The user submits a query, which is first processed by a retriever.</p></li>
<li><p>The retriever searches a knowledge base or database for the most relevant information. This retrieval step can use various methods, such as embedding similarity or full-text search, to find the best matches.</p></li>
<li><p>The retrieved information is then added to the model’s context, allowing the generation step to produce an informed response based on both the query and the retrieved information.</p></li>
</ol>
<p>RAG can make the model less prone to hallucinations—or generating inaccurate information— and also makes it easier for it to correctly reference the sources of its outputs, something that pure LLM generations often struggle with.
The performance RAG is limited by the accuracy and the latency of the retrieval system, so compared to LLM-only generation, it becomes more challenging to get every component of this system to work accurately.</p>
</section>
<section id="tool-use-and-function-calling">
<h3>Tool Use and Function Calling<a class="headerlink" href="#tool-use-and-function-calling" title="Link to this heading">#</a></h3>
<p>Tool use and function calling is a natural extension to Retrieval-Augmented Generation (RAG).
While RAG allows models to retrieve relevant information, some tasks require real-time data, precise calculations, or specialized functions that go beyond retrieval.
With tool use and function calling, language models can dynamically call external tools—such as calculators, web searches, code interpreters, or APIs—to enhance their responses.</p>
<p><img alt="tool-use" src="../_images/tool-use.png" /></p>
<p>When a user submits a query, the model can decide whether it needs to invoke a specific tool, determined by whether it samples a special token that indicates it should use a tool or call a function.
If so, it dispatches a call to the relevant tool, retrieves the result, and integrates it into the response.
This capability enables models to go beyond static knowledge and also to make changes to the external world by executing code or calling APIs.</p>
<p>In ChatGPT or Claude, you can make it execute the code it generated and show the results, or more recently, you make the LLM take control of a virtual desktop computer and perform tasks using it.
This is enabling the emergent field of LLM agents, and it is an open challenge to make multiple intelligent agents to interact with each other and achieve complex goals.</p>
</section>
<section id="multimodal-encoders-for-language-model-inputs">
<h3>Multimodal Encoders for Language Model Inputs<a class="headerlink" href="#multimodal-encoders-for-language-model-inputs" title="Link to this heading">#</a></h3>
<p>To incorporate image or audio data into language models, we can use multimodal encoders.</p>
<p>The idea is to feed image or audio embeddings, instead of just word embeddings, into the model.
The encoder can be anything, from supervised, self-supervised, or contrastive model, as long as it can provide a good representation for that modality.
A common choice is to pick from openly available pre-trained models, such as CLIP for image inputs and Whisper for speech inputs.
Alternatively, the encoder can also be trained from scratch, although it generally requires more computational resources and data.</p>
<p>Multimodal language models allow inputs that have texts, images, and audio interleaved, and the language model takes the encoded features of multimodal inputs.</p>
<p>The Flamingo model used cross attention to attend on image features, but it can be any other form of conditioning. We can be very flexible with conditioning inputs, as we saw earlier.</p>
</section>
<section id="multimodal-decoders-for-language-model-outputs">
<h3>Multimodal Decoders for Language Model Outputs<a class="headerlink" href="#multimodal-decoders-for-language-model-outputs" title="Link to this heading">#</a></h3>
<p>But for multimodal outputs, we need to be more clever.
Unlike with text, typical language models can only generate discrete tokens, which isn’t ideal for images or audio.
To handle this, we have a few approaches.</p>
<p>An early approach is to predict discretized pixel or amplitude values directly, as in models like ImageGPT for images or WaveNet for audio.
A more common method recently is to use vector-quantized variational autoencoders (VQ-VAEs), which allow the model to generate sequences of discrete codes that represent an image or audio signal, as seen in DALL-E. These codes are then decoded back into the original modality.</p>
<p>However, VAE outputs can sometimes be blurry, so to improve quality, we often use GANs or diffusion models as better VQ decoders—as in VQGAN, Encodec, or Sora.</p>
<p><a class="reference internal" href="#../generation/diffusionmodel.html"><span class="xref myst">Later in this tutorial</span></a>, Zackery and Ke will talk more on this direction of approaches for music generation.</p>
</section>
<section id="chain-of-thought-reasoning-of-language-models">
<h3>Chain-of-Thought Reasoning of Language Models<a class="headerlink" href="#chain-of-thought-reasoning-of-language-models" title="Link to this heading">#</a></h3>
<p>Language models often produce quick answers without thinking through the problem, or spending more compute proportional to the difficulty of the given problem, which can lead to incorrect responses, especially on complex questions.
<strong>Chain-of-Thought (CoT) prompting</strong> addresses this by encouraging the model to reason step-by-step.</p>
<p><img alt="cot" src="../_images/cot.png" /></p>
<p>In CoT prompting, we give the model examples with in-context reasoning, such as samples of step-by-step solutions to math problems.
In the example on the left, the language model fails to solve a math problem when prompted with a simple list of questions and answers in the context,
but it correctly solves the problem when the in-context examples included step-by-step reasoning.
Interestingly, it’s also been shown that by just adding “Let’s think step by step” leads to more accurate results, without having to provide any in-context few-shot examples.</p>
<p>Going further than just clever prompting, we can use reinforcement learning to optimize the model to generate better chains of thoughts that are more likely to arrive at the correct answer. This is the basic idea behind the recent OpenAI o1 model, where we showed that the model’s performance predictably increases as the model spends more compute for sampling longer chains of thoughts, hinting at the scaling law of inference-time compute, which can orthogonally improve language models on top of the train-time compute.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lm"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="framework.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Framework</p>
      </div>
    </a>
    <a class="right-next"
       href="challenges.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Challenges</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning-from-language-models">Transfer Learning from Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-task-transfer-and-in-context-learning">Zero-shot Task Transfer and In-Context Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-laws-of-language-models">Scaling Laws of Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distillation-of-language-models">Distillation of Language Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aligning-language-models-with-human-feedback">Aligning Language Models with Human Feedback</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-use-and-function-calling">Tool Use and Function Calling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-encoders-for-language-model-inputs">Multimodal Encoders for Language Model Inputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal-decoders-for-language-model-outputs">Multimodal Decoders for Language Model Outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-of-thought-reasoning-of-language-models">Chain-of-Thought Reasoning of Language Models</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim, Ke Chen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>